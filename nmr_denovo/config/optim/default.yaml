optimizer:
  #  Adam-oriented deep learning
  _target_: torch.optim.AdamW
  #  These are all default parameters for the Adam optimizer
  lr: 0.0001
  # betas: [ 0.9, 0.999 ]
  # eps: 1.0e-08
  weight_decay: 1e-2

use_lr_scheduler: True

warmup_ratio: 0.02
max_epochs: ${trainer.max_epochs}

lr_scheduler:
  _target_: src.common.scheduler.PolyLR
  # warmup: 2
  # max_iters: 200


